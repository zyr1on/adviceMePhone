import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Etiket kategorileri ve karÅŸÄ±tlarÄ±
conflicting_groups = [
    {"good_camera", "bad_camera"},
    {"apple", "android"},
    {"samsung", "ios"},
    {"ios", "android"},
    # Buraya sen etiketlerine gÃ¶re istediÄŸin kadar ekleyebilirsin
]

def add_label_with_conflict_check(predicted_labels, new_label):
    for group in conflicting_groups:
        if new_label in group:
            # AynÄ± gruptaki diÄŸer etiketleri Ã§Ä±kar
            for label in list(predicted_labels):
                if label in group and label != new_label:
                    predicted_labels.remove(label)
    # Yeni etiketi ekle
    if new_label not in predicted_labels:
        predicted_labels.append(new_label)


# ======================== #
# 1. cihaz ayarÄ± (cpu / gpu)
# ======================== # 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ======================== #
# etiketleri yÃ¼kleyelim (label.txt)
# ======================== #

with open("labels.txt", "r", encoding="utf-8") as f:
    raw = f.read() 

# ======================== #
# - good, bad gibi etiketleri bulmak iÃ§in regex kullanÄ±yoruz
# ======================== #
import re
labels = re.findall(r"-(.+)", raw) #r regex ile etiketleri bul, "-(.+)" ile - ile baÅŸlayan ve sonrasÄ±nda gelen her ÅŸeyi al
labels = sorted(set([l.strip() for l in labels]))  # etiketleri temizle, baÅŸÄ±ndaki ve sonundaki boÅŸluklarÄ± kaldÄ±r
print(labels)  # Etiketlerin doÄŸru ÅŸekilde yÃ¼klendiÄŸini kontrol edin

# ======================== #
#  sayisal id eslemeleri :
# ======================== #

id2label = {i: label for i, label in enumerate(labels)}  # id'den etikete
print(id2label)  # Etiketlerin doÄŸru ÅŸekilde eÅŸleÅŸtirildiÄŸini kontrol edin

# ======================== #
# 3. tokenizer ve egtitilen modeli yÃ¼kle    
# ======================== #
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
from transformers import DistilBertConfig

# Model yapÄ±landÄ±rmasÄ±nÄ± yeniden tanÄ±mla
config = DistilBertConfig(
    num_labels=len(labels),
    problem_type="multi_label_classification"
)

# YapÄ±landÄ±rmaya uygun boÅŸ model oluÅŸtur
model = DistilBertForSequenceClassification(config)


# ======================== #
# egitilmiÅŸ agÄ±rlÄ±klarÄ± yÃ¼klÃ¼yoruz
# ======================== #

model.load_state_dict(torch.load("model.pt", map_location=device)) # modelin aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle
model.to(device) # modeli cihaza taÅŸÄ±yoruz
model.eval() # Modeli deÄŸerlendirme moduna al

# ======================== #
#   kullanÄ±cÄ±dan cÃ¼mle al ve tahmin et
# ======================== #

cumle = input("KullanÄ±cÄ±dan cÃ¼mlesi:")

# TOKENÄ°ZE EDÄ°YOZZZ

encoded = tokenizer.encode_plus(
    cumle,
    add_special_tokens=True,  # [CLS] ve [SEP] ekle CLS = baÅŸlangÄ±Ã§ tokeni, SEP = cÃ¼mle sonu tokeni
    padding='max_length',  # Maksimum uzunlÄ±ÄŸa kadar doldur
    truncation=True,  # Ã‡ok uzun cÃ¼mleleri kes
    max_length=32,  # Maksimum token sayÄ±sÄ±
    return_tensors="pt"  # PyTorch tensÃ¶r olarak dÃ¶ndÃ¼r
)

input_ids = encoded['input_ids'].to(device)  # Token ID'lerini cihaza taÅŸÄ±
attention_mask = encoded["attention_mask"].to(device)  # Padding maskesini cihaza taÅŸÄ±
# padding maskesi, hangi tokenlarÄ±n gerÃ§ek olduÄŸunu gÃ¶sterir (1 = gerÃ§ek, 0 = pad)

with torch.no_grad():  # DeÄŸerlendirme sÄ±rasÄ±nda gradyan hesaplamaya gerek yok
    #gradyan hesaplama bu iÅŸlemde gerekli deÄŸil, Ã§Ã¼nkÃ¼ model sadece tahmin yapacak
    outputs=model(input_ids=input_ids, attention_mask=attention_mask)  # Modeli Ã§alÄ±ÅŸtÄ±r
    logits = outputs.logits  # Modelin Ã§Ä±ktÄ±sÄ± (logit deÄŸerleri)
    probs = torch.sigmoid(logits)[0]
    print(probs)  # Modelin dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ olasÄ±lÄ±k deÄŸerlerini kontrol edin

# ======================== #
# belirli eÅŸik deÄŸeri (threshold) Ã¼stÃ¼nedeki etiketleri al
# ======================== #
THRESHOLD = 0.41  # EÅŸik deÄŸeri
predicted_labels = [id2label[i] for i, prob in enumerate(probs) if prob > THRESHOLD]

#sonucu yazdÄ±r
print("\n tahmin edilen etiketlerğŸ§ :")
if predicted_labels: 
    for tag in predicted_labels:
        print(f"- {tag}")
else:
    print("Etiket bulunamadÄ±. EÅŸik deÄŸerinin altÄ±nda kalan etiketler var.")

# ======================== #
# kullanÄ±cÄ±dan bilgi alma 
# ======================== #

print("\nNe tarz bir telefon arÄ±yorsunuz? (Ã¶rn: iyi kamera, uzun pil Ã¶mrÃ¼)")
cumle = input("LÃ¼tfen belirtiniz: ")

# TOKENÄ°ZE EDÄ°YOZZZ

encoded = tokenizer.encode_plus( #burada kullanÄ±cÄ±dan gelen cÃ¼mleyi tokenize ediyoruz
    cumle,
    add_special_tokens=True,
    padding="max_length",
    truncation=True, # Ã‡ok uzun cÃ¼mleleri kes
    max_length=32,  # Maksimum token sayÄ±sÄ±
    return_tensors="pt"  # PyTorch tensÃ¶r olarak dÃ¶ndÃ¼r
)

input_ids = encoded["input_ids"].to(device)  # Token ID'lerini cihaza taÅŸÄ±
attention_mask = encoded["attention_mask"].to(device)  # Padding maskesini cihaza taÅŸÄ±

# tahmin etme kÄ±smÄ±

with torch.no_grad():  # DeÄŸerlendirme sÄ±rasÄ±nda gradyan hesaplamaya gerek yok
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Modeli Ã§alÄ±ÅŸtÄ±r
    logits = outputs.logits 
    probs = torch.sigmoid(logits)[0]  # OlasÄ±lÄ±k deÄŸerlerini hesapla

    print("\OlasÄ±lÄ±klar(etiket:olasÄ±lÄ±k):")
    for i, prob in enumerate(probs):
        print(f"{id2label[i]}:{prob.item():.4f}")

# ========================= #
# threshold'a gÃ¶re tahmin etiketleri: 
# ========================= #

THRESHOLD = 0.41 
predicted_labels = [id2label[i] for i, prob in enumerate(probs) if prob > THRESHOLD]

# ========================= #                            |
# etiketleri kontrol et hiÃ§ etiket yoksa soru sorucaz   \|/
# ========================= #                            V
print("\nTahmin edilen etiketler:")

# EÄŸer tahmin edilen etiket sayÄ±sÄ± 3'ten azsa, kullanÄ±cÄ±dan ek bilgi ister
while len(predicted_labels) < 3:
    cumle = input("Yeterli bilgi alÄ±namadÄ±. Daha fazla detay verebilir misiniz?\nâ†’ ")

    # KullanÄ±cÄ± cÃ¼mlesini tokenize et
    encoded = tokenizer.encode_plus(
        cumle,
        add_special_tokens=True,
        padding='max_length',
        truncation=True,
        max_length=32,
        return_tensors="pt"
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    # Modeli kullanarak tahmin yap (gradyan hesaplama kapalÄ±)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = torch.sigmoid(logits)[0]

    # Tahmin edilen etiketleri gÃ¼ncelle
    for i, prob in enumerate(probs):
        if prob > THRESHOLD:
            tag = id2label[i]
            add_label_with_conflict_check(predicted_labels, tag)



    # GÃ¼ncellenen etiketleri yazdÄ±r
    print("\nGÃ¼ncellenmiÅŸ tercihler:")
    for tag in predicted_labels:
        print(f"- {tag}")

# DÃ¶ngÃ¼den Ã§Ä±kÄ±nca tÃ¼m etiketleri yazdÄ±rabiliriz
print("\nSon tahmin edilen etiketler:")
for tag in predicted_labels:
    print(f"- {tag}")

# ========================= #                            ^
# etiketleri kontrol et hiÃ§ etiket yoksa soru sorucaz   /|\
# ========================= #                            |


        
    print("\nGÃ¼ncellenmiÅŸ tahmin edilen etiketler:")
    for tag in predicted_labels:
        print(f"- {tag}")

elif len(predicted_labels) < 2: 
    print(f"\nÅuana kadar algÄ±lanan Ã¶zellikler: {predicted_labels}")
    print("BirkaÃ§ Ã–zellik daha ekleyebilir misiniz? (Ã¶rn: iyi kamera, uzun pil Ã¶mrÃ¼)")
    # burada devam
else:
    print("\nVerdiÄŸiniz bilgiler iÃ§in teÅŸekkÃ¼r ederim. Åimdi size uygun telefonlarÄ± bulmaya Ã§alÄ±ÅŸacaÄŸÄ±m.")
    print("\nAlgÄ±lanan Telefon Ã¶zellikleri:")
    for tag in predicted_labels:
        print(f"- {tag}")
