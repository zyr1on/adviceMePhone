import torch 
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.optim import AdamW


import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"EÄŸitim cihazÄ±: {device}")

# DosyayÄ± yÃ¼kle
dataset = torch.load("dataset.pt", weights_only=False)


batch_size = 8 
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

num_labels = dataset.tensors[2].shape[1]  # Etiket sayÄ±sÄ±
print(f"Etiket sayÄ±sÄ±: {num_labels}")

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=num_labels,  # Multi-label iÃ§in etiket sayÄ±sÄ±
    problem_type="multi_label_classification"  # Multi-label sÄ±nÄ±flandÄ±rma
)

model.to(device)  # Modeli CUDA'ya taÅŸÄ±

# === KayÄ±p fonksiyonu ve optimizer ===
criterion = nn.BCEWithLogitsLoss()  # Ã‡oklu etiket iÃ§in Ã¶zel loss
optimizer = AdamW(model.parameters(), lr=2e-5)  # BERT iÃ§in Ã¶nerilen Ã¶ÄŸrenme oranÄ±


# === EÄŸitim dÃ¶ngÃ¼sÃ¼ ===
epochs = 5

for epoch in range(epochs):
    print(f"\nğŸ” Epoch {epoch+1}/{epochs}")
    model.train()
    total_loss = 0

    for batch in loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        logits = outputs.logits  # Modelin tahmini (Ã§Ä±ktÄ±sÄ±)
        loss = criterion(logits, labels.float())  # KayÄ±p deÄŸeri

        optimizer.zero_grad()  # Ã–nceki gradyanlarÄ± sÄ±fÄ±rla
        loss.backward()        # Geri yayÄ±lÄ±m
        optimizer.step()       # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle

        total_loss += loss.item()

    avg_loss = total_loss / len(loader)
    print(f"ğŸ“‰ Ortalama epoch kaybÄ±: {avg_loss:.4f}")

# === Modeli kaydet ===
torch.save(model.state_dict(), "model.pt")
print("âœ… Model baÅŸarÄ±yla eÄŸitildi ve 'model.pt' olarak kaydedildi.")